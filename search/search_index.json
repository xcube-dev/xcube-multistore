{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"xcube Multi-Source Data Store","text":"<p><code>xcube-multistore</code> is a Python package designed to create a Multi-Source Data Store  that enables the seamless integration of data from multiple sources into a unified  data model. This approach simplifies the data fusion process while ensuring  transparency and reproducibility through well-defined configurations.</p> <p>The package utilizes xcube\u2019s data access,  implemented via data store plugins, along with additional functionalities from  xcube, to manipulate and harmonize datasets according to user-defined specifications.</p> <p>The workflow includes the following steps:</p> <ol> <li>Data access through xcube data stores</li> <li>Data harmonization</li> <li>Data fusion (if required)</li> </ol> <p>This process results in either a single, unified data cube with all datasets aligned to a consistent grid or a catalog of separate datasets.</p>"},{"location":"#overview","title":"Overview","text":"<p>The Multi-Source Data Store is configured via a YAML file. You can find an example  configuration in examples/config.yml.</p> <p>For more detailed guidance on creating a configuration file, please refer to the  Configuration Guide.</p> <p>Once the configuration file is ready, the Multi-Source Data Store can be started  with a single line of code, as shown below:</p> <pre><code>from xcube_multistore.multistore import MultiSourceDataStore\n\nmsds = MultiSourceDataStore(\"config.yml\")\n</code></pre> <p>For further examples please view the examples folder.</p>"},{"location":"#features","title":"Features","text":"<p>IMPORTANT: The <code>xcube-multistore</code> package is currently in the early stages of development. The following features are available so far:</p> <ul> <li>subset of dataset (defined by grid mapping)</li> <li>resample and reproject dataset (defined by grid mapping)</li> <li>grid mapping may be defined by the user or by a dataset </li> <li>allow for time series at a single spatial point; interpolate the neighbouring points</li> <li>allow data fusion, where data variables in one <code>xr.Dataset</code> refers to different data sources</li> <li>support spatial cutout of an area around a defined spatial point.</li> <li>support preload API for xcube-clms and    xcube-zendoo</li> <li>allow to write to netcdf and zarr</li> </ul> <p>The following features will be implemented in the future:</p> <ul> <li>some auxiliary functionalities which shall help to setup a config YAML file. </li> <li>interpolate along the time axis </li> </ul>"},{"location":"#license","title":"License","text":"<p>The package is open source and released under the  MIT license. </p>"},{"location":"about/","title":"About the <code>xcube-multistore</code> project","text":""},{"location":"about/#changelog","title":"Changelog","text":"<p>You can find the complete <code>xcube-multistore</code> changelog  here. </p>"},{"location":"about/#reporting","title":"Reporting","text":"<p>If you have suggestions, ideas, feature requests, or if you have identified a malfunction or error, then please  post an issue. </p>"},{"location":"about/#contributions","title":"Contributions","text":"<p>The <code>xcube-multistore</code> project welcomes contributions of any form as long as you respect our  code of conduct and follow our  contribution guide.</p> <p>If you'd like to submit code or documentation changes, we ask you to provide a  pull request (PR)  here.  For code and configuration changes, your PR must be linked to a  corresponding issue. </p>"},{"location":"about/#development","title":"Development","text":"<p>To install the <code>xcube-multistore</code> development environment into an existing Python  environment, do</p> <pre><code>pip install .[dev,doc]\n</code></pre> <p>or create a new environment using <code>conda</code> or <code>mamba</code></p> <pre><code>mamba env create \n</code></pre>"},{"location":"about/#testing-and-coverage","title":"Testing and Coverage","text":"<p><code>xcube-multistore</code> uses pytest for unit-level testing  and code coverage analysis.</p> <pre><code>pytest tests/ --cov=xarray_eopf --cov-report html\n</code></pre>"},{"location":"about/#code-style","title":"Code Style","text":"<p>The <code>xcube-multistore</code> source code is formatted and quality-controlled  using ruff:</p> <pre><code>ruff format\nruff check\n</code></pre>"},{"location":"about/#documentation","title":"Documentation","text":"<p>The <code>xcube-multistore</code> documentation is built using the  mkdocs tool.</p> <p>With repository root as current working directory:</p> <pre><code>pip install .[doc]\n\nmkdocs build\nmkdocs serve\nmkdocs gh-deploy\n</code></pre>"},{"location":"about/#license","title":"License","text":"<p><code>xcube-multistore</code> is open source made available under the terms and conditions of the  MIT license.</p>"},{"location":"api/","title":"Python API reference","text":""},{"location":"api/#xcube_multistore.multistore.MultiSourceDataStore","title":"<code>xcube_multistore.multistore.MultiSourceDataStore</code>","text":"<p>Manages access to multiple data sources and their configurations for generating data cubes.</p> <p>This class utilizes xcube data store plugins for data access, supports data harmonization, and enables visualization of data cube generation.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str | dict[str, Any]</code> <p>Configuration settings, provided as a dictionary or a string reference to a YAML configuration file.</p> required Notes <p>Detailed instructions on setting up the configuration can be found in the Configuration Guide.</p> Source code in <code>xcube_multistore/multistore.py</code> <pre><code>class MultiSourceDataStore:\n    \"\"\"Manages access to multiple data sources and their configurations for generating\n    data cubes.\n\n    This class utilizes xcube data store plugins for data access, supports data\n    harmonization, and enables visualization of data cube generation.\n\n    Args:\n        config: Configuration settings, provided as a dictionary or a string\n            reference to a YAML configuration file.\n\n    Notes:\n        Detailed instructions on setting up the configuration can be found in the\n        [Configuration Guide](https://xcube-dev.github.io/xcube-multistore/config/).\n    \"\"\"\n\n    def __init__(self, config: str | dict[str, Any]):\n        config = MultiSourceConfig(config)\n        self.config = config\n        self.stores = DataStores.setup_data_stores(config)\n        if config.grid_mappings:\n            self._grid_mappings = GridMappings.setup_grid_mappings(config)\n        else:\n            self._grid_mappings = None\n        self._states = {\n            identifier: GeneratorState(\n                identifier=identifier, status=GeneratorStatus.waiting\n            )\n            for identifier, config_ds in config.datasets.items()\n        }\n\n        # preload data, which is not preloaded as default\n        if config.preload_datasets is not None:\n            self._preload_datasets()\n\n        # generate data cubes\n        if self.config.general[\"visualize\"]:\n            self._display = GeneratorDisplay.create(list(self._states.values()))\n            self._display.display_title(\"Cube Generation\")\n            self._display.show()\n        self._generate_cubes()\n\n    @classmethod\n    def get_config_schema(cls) -&gt; JsonObjectSchema:\n        \"\"\"Retrieves the configuration schema for the multi-source data store.\n\n        Returns:\n            A schema object defining the expected structure of the configuration.\n        \"\"\"\n        return MultiSourceConfig.get_schema()\n\n    def _notify(self, event: GeneratorState):\n        state = self._states[event.identifier]\n        state.update(event)\n        if self.config.general[\"visualize\"]:\n            self._display.update()\n        else:\n            if event.status == GeneratorStatus.failed:\n                LOG.error(\"An error occurred: %s\", event.exception)\n            else:\n                LOG.info(event.message)\n\n    def _notify_error(self, identifier: str, exception: Any):\n        self._notify(\n            GeneratorState(\n                identifier,\n                status=GeneratorStatus.failed,\n                exception=exception,\n            )\n        )\n\n    def _preload_datasets(self):\n        for config_preload in self.config.preload_datasets:\n            store = getattr(self.stores, config_preload[\"store\"])\n\n            if self.config.general[\"force_preload\"]:\n                # preload all datasets again\n                data_ids_preloaded = []\n                data_ids = config_preload[\"data_ids\"]\n            else:\n                # filter preloaded data IDs\n                data_ids = []\n                data_ids_preloaded = []\n                for data_id_preload in config_preload[\"data_ids\"]:\n                    if all(\n                        store.cache_store.has_data(data_id)\n                        for data_id in self.config.preload_map[data_id_preload]\n                    ):\n                        data_ids_preloaded.append(data_id_preload)\n                    else:\n                        data_ids.append(data_id_preload)\n\n            # setup visualization\n            if self.config.general[\"visualize\"]:\n                display_preloaded = GeneratorDisplay.create(\n                    [\n                        GeneratorState(\n                            identifier=data_id,\n                            status=GeneratorStatus.stopped,\n                            message=\"Already preloaded.\",\n                        )\n                        for data_id in data_ids_preloaded\n                    ]\n                )\n                display_preloaded.display_title(\n                    f\"Preload Datasets from store {config_preload['store']!r}\"\n                )\n                if data_ids_preloaded:\n                    display_preloaded.show()\n            else:\n                LOG.info(f\"Preload Datasets from store {config_preload['store']!r}\")\n                for data_id in data_ids_preloaded:\n                    LOG.info(f\"Data ID {data_id!r} already preloaded.\")\n\n            if data_ids:\n                preload_params = config_preload.get(\"preload_params\", {})\n                if \"silent\" not in preload_params:\n                    preload_params[\"silent\"] = self.config.general[\"visualize\"]\n                _ = store.preload_data(*data_ids, **preload_params)\n\n    def _generate_cubes(self):\n        for identifier, config_ds in self.config.datasets.items():\n            data_id = _get_data_id(config_ds)\n            if getattr(self.stores, \"storage\").has_data(data_id):\n                self._notify(\n                    GeneratorState(\n                        identifier,\n                        status=GeneratorStatus.stopped,\n                        message=f\"Dataset {identifier!r} already generated.\",\n                    )\n                )\n                continue\n            self._notify(\n                GeneratorState(\n                    identifier,\n                    status=GeneratorStatus.started,\n                    message=f\"Open dataset {identifier!r}.\",\n                )\n            )\n            ds = self._open_dataset(config_ds)\n            if isinstance(ds, xr.Dataset):\n                self._notify(\n                    GeneratorState(\n                        identifier,\n                        message=f\"Processing dataset {identifier!r}.\",\n                    )\n                )\n            else:\n                self._notify_error(identifier, ds)\n                continue\n            ds = self._process_dataset(ds, config_ds)\n            if isinstance(ds, xr.Dataset):\n                self._notify(\n                    GeneratorState(\n                        identifier,\n                        message=f\"Write dataset {identifier!r}.\",\n                    )\n                )\n            else:\n                self._notify_error(identifier, ds)\n                continue\n            ds = self._write_dataset(ds, config_ds)\n            if isinstance(ds, xr.Dataset):\n                self._notify(\n                    GeneratorState(\n                        identifier,\n                        status=GeneratorStatus.stopped,\n                        message=f\"Dataset {identifier!r} finished.\",\n                    )\n                )\n            else:\n                store = getattr(self.stores, NAME_WRITE_STORE)\n                format_id = config_ds.get(\"format_id\", \"zarr\")\n                data_id = (\n                    f\"{config_ds['identifier']}.{MAP_FORMAT_ID_FILE_EXT[format_id]}\"\n                )\n                store.has_data(data_id) and store.delete_data(data_id)\n                self._notify_error(identifier, ds)\n\n    @_safe_execute()\n    def _open_dataset(self, config: dict) -&gt; xr.Dataset | Exception:\n        if \"data_id\" in config:\n            return self._open_single_dataset(config)\n        else:\n            dss = []\n            for config_var in config[\"variables\"]:\n                ds = self._open_single_dataset(config_var)\n                if len(ds.data_vars) &gt; 1:\n                    name_dict = {\n                        var: f\"{config_var[\"identifier\"]}_{var}\"\n                        for var in ds.data_vars.keys()\n                    }\n                else:\n                    name_dict = {\n                        var: f\"{config_var[\"identifier\"]}\"\n                        for var in ds.data_vars.keys()\n                    }\n                dss.append(ds.rename_vars(name_dict=name_dict))\n            merge_params = config.get(\"xr_merge_params\", {})\n            if \"join\" not in merge_params:\n                merge_params[\"join\"] = \"exact\"\n            if \"combine_attrs\" not in merge_params:\n                merge_params[\"combine_attrs\"] = \"drop_conflicts\"\n            ds = xr.merge(dss, **merge_params)\n        return clean_dataset(ds)\n\n    def _open_single_dataset(self, config: dict) -&gt; xr.Dataset | Exception:\n        store = getattr(self.stores, config[\"store\"])\n        open_params = copy.deepcopy(config.get(\"open_params\", {}))\n        lat, lon = open_params.pop(\"point\", [np.nan, np.nan])\n        schema = store.get_open_data_params_schema(data_id=config[\"data_id\"])\n        if (\n            ~np.isnan(lat)\n            and ~np.isnan(lon)\n            and \"bbox\" in schema.properties\n            and [\"spatial_res\"] in open_params\n            and \"spatial_res\" in schema.properties\n        ):\n            lat, lon = open_params.pop(\"point\")\n            open_params[\"bbox\"] = [\n                lon - 2 * open_params[\"spatial_res\"],\n                lat - 2 * open_params[\"spatial_res\"],\n                lon + 2 * open_params[\"spatial_res\"],\n                lat + 2 * open_params[\"spatial_res\"],\n            ]\n\n        if hasattr(store, \"cache_store\"):\n            try:\n                ds = store.cache_store.open_data(config[\"data_id\"], **open_params)\n            except Exception:\n                ds = store.open_data(config[\"data_id\"], **open_params)\n        else:\n            ds = store.open_data(config[\"data_id\"], **open_params)\n\n        # custom processing\n        if \"custom_processing\" in config:\n            module = importlib.import_module(config[\"custom_processing\"][\"module_path\"])\n            function = getattr(module, config[\"custom_processing\"][\"function_name\"])\n            ds = function(ds)\n\n        return clean_dataset(ds)\n\n    @_safe_execute()\n    def _process_dataset(self, ds: xr.Dataset, config: dict) -&gt; xr.Dataset | Exception:\n        # if grid mapping is given, resample the dataset\n        if \"grid_mapping\" in config:\n            if hasattr(self._grid_mappings, config[\"grid_mapping\"]):\n                target_gm = getattr(self._grid_mappings, config[\"grid_mapping\"])\n            else:\n                config_ref = self.config.datasets[config[\"grid_mapping\"]]\n                data_id = _get_data_id(config_ref)\n                ds_ref = getattr(self.stores, \"storage\").open_data(data_id)\n                target_gm = GridMapping.from_dataset(ds_ref)\n                for var_name, data_array in ds.items():\n                    if np.issubdtype(data_array.dtype, np.number):\n                        ds[var_name] = data_array.astype(target_gm.x_coords.dtype)\n            source_gm = GridMapping.from_dataset(ds)\n            transformer = pyproj.Transformer.from_crs(\n                target_gm.crs, source_gm.crs, always_xy=True\n            )\n            bbox = transformer.transform_bounds(*target_gm.xy_bbox, densify_pts=21)\n            bbox = [\n                bbox[0] - 2 * source_gm.x_res,\n                bbox[1] - 2 * source_gm.y_res,\n                bbox[2] + 2 * source_gm.x_res,\n                bbox[3] + 2 * source_gm.y_res,\n            ]\n\n            ds = clip_dataset_by_geometry(ds, geometry=bbox)\n            ds = resample_in_space(ds, target_gm=target_gm, encode_cf=True)\n            # this is needed since resample in space returns one chunk along the time\n            # axis; this part can be removed once https://github.com/xcube-dev/xcube/issues/1124\n            # is resolved.\n            if \"time\" in ds.coords:\n                ds = chunk_dataset(\n                    ds, dict(time=1), format_name=config.get(\"format_id\", \"zarr\")\n                )\n\n        # if \"point\" in open_params, timeseries is requested\n        open_params = config.get(\"open_params\", {})\n        if \"point\" in open_params:\n            ds = ds.interp(\n                lat=open_params[\"point\"][0],\n                lon=open_params[\"point\"][1],\n                method=\"linear\",\n            )\n\n        return ds\n\n    @_safe_execute()\n    def _write_dataset(self, ds: xr.Dataset, config: dict) -&gt; xr.Dataset | Exception:\n        store = getattr(self.stores, NAME_WRITE_STORE)\n        format_id = config.get(\"format_id\", \"zarr\")\n        if format_id == \"netcdf\":\n            ds = prepare_dataset_for_netcdf(ds)\n        data_id = f\"{config['identifier']}.{MAP_FORMAT_ID_FILE_EXT[format_id]}\"\n        ds = clean_dataset(ds)\n        store.write_data(ds, data_id, replace=True)\n        return ds\n</code></pre>"},{"location":"api/#xcube_multistore.multistore.MultiSourceDataStore.get_config_schema","title":"<code>get_config_schema()</code>  <code>classmethod</code>","text":"<p>Retrieves the configuration schema for the multi-source data store.</p> <p>Returns:</p> Type Description <code>JsonObjectSchema</code> <p>A schema object defining the expected structure of the configuration.</p> Source code in <code>xcube_multistore/multistore.py</code> <pre><code>@classmethod\ndef get_config_schema(cls) -&gt; JsonObjectSchema:\n    \"\"\"Retrieves the configuration schema for the multi-source data store.\n\n    Returns:\n        A schema object defining the expected structure of the configuration.\n    \"\"\"\n    return MultiSourceConfig.get_schema()\n</code></pre>"},{"location":"api/#xcube_multistore.utils.prepare_dataset_for_netcdf","title":"<code>xcube_multistore.utils.prepare_dataset_for_netcdf(ds)</code>","text":"<p>Prepares an xarray Dataset for NetCDF serialization.</p> <p>Converts non-serializable attributes (lists, tuples, and dictionaries) into strings to ensure compatibility with NetCDF format.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input xarray Dataset.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>A dataset with updated attributes, ensuring compatibility with NetCDF.</p> Source code in <code>xcube_multistore/utils.py</code> <pre><code>def prepare_dataset_for_netcdf(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Prepares an xarray Dataset for NetCDF serialization.\n\n    Converts non-serializable attributes (lists, tuples, and dictionaries) into strings\n    to ensure compatibility with NetCDF format.\n\n    Args:\n        ds: The input xarray Dataset.\n\n    Returns:\n        A dataset with updated attributes, ensuring compatibility with NetCDF.\n    \"\"\"\n    attrs = ds.attrs\n    for key in attrs:\n        if (\n            isinstance(attrs[key], list)\n            or isinstance(attrs[key], tuple)\n            or isinstance(attrs[key], dict)\n        ):\n            attrs[key] = str(attrs[key])\n    ds = ds.assign_attrs(attrs)\n    return ds\n</code></pre>"},{"location":"api/#xcube_multistore.utils.get_utm_zone","title":"<code>xcube_multistore.utils.get_utm_zone(lat, lon)</code>","text":"<p>Determines the UTM (Universal Transverse Mercator) zone for given coordinates.</p> <p>Computes the UTM zone based on longitude and returns the corresponding EPSG code. Northern hemisphere zones use EPSG codes in the 32600 range, while southern hemisphere zones use EPSG codes in the 32700 range.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude in degrees.</p> required <code>lon</code> <code>float</code> <p>Longitude in degrees.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The EPSG code for the corresponding UTM zone (e.g., \"epsg:32633\").</p> Source code in <code>xcube_multistore/utils.py</code> <pre><code>def get_utm_zone(lat: float, lon: float) -&gt; str:\n    \"\"\"Determines the UTM (Universal Transverse Mercator) zone for given coordinates.\n\n    Computes the UTM zone based on longitude and returns the corresponding EPSG code.\n    Northern hemisphere zones use EPSG codes in the 32600 range, while southern\n    hemisphere zones use EPSG codes in the 32700 range.\n\n    Args:\n        lat: Latitude in degrees.\n        lon: Longitude in degrees.\n\n    Returns:\n        The EPSG code for the corresponding UTM zone (e.g., \"epsg:32633\").\n    \"\"\"\n    zone_number = int((lon + 180) / 6) + 1\n    if lat &gt;= 0:\n        epsg_code = 32600 + zone_number\n    else:\n        epsg_code = 32700 + zone_number\n    return f\"epsg:{epsg_code}\"\n</code></pre>"},{"location":"api/#xcube_multistore.utils.get_bbox","title":"<code>xcube_multistore.utils.get_bbox(lat, lon, cube_width, crs_final='utm')</code>","text":"<p>Generates a bounding box around a specified latitude and longitude.</p> <p>Given a point (latitude, longitude) and the desired width of a cube, this function computes the bounding box in the specified coordinate reference system (CRS). The bounding box is returned as a list of coordinates, and the CRS is returned as well.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of the central point in degrees.</p> required <code>lon</code> <code>float</code> <p>Longitude of the central point in degrees.</p> required <code>cube_width</code> <code>float</code> <p>The width of the cube in units of crs_final, used to define the extent of the bounding box.</p> required <code>crs_final</code> <code>CRS or str</code> <p>The target CRS for the bounding box. Defaults to \"utm\", which automatically determines the UTM zone based on the latitude and longitude.</p> <code>'utm'</code> <p>Returns:</p> Type Description <code>(list[int], CRS)</code> <p>A list of four integers representing the bounding box in the format [west, south, east, north].</p> <code>(list[int], CRS)</code> <p>The final CRS used for the bounding box, returned as a <code>pyproj.CRS</code> object.</p> Source code in <code>xcube_multistore/utils.py</code> <pre><code>def get_bbox(\n    lat: float, lon: float, cube_width: float, crs_final: pyproj.CRS | str = \"utm\"\n) -&gt; (list[int], pyproj.CRS):\n    \"\"\"Generates a bounding box around a specified latitude and longitude.\n\n    Given a point (latitude, longitude) and the desired width of a cube, this function\n    computes the bounding box in the specified coordinate reference system (CRS).\n    The bounding box is returned as a list of coordinates, and the CRS is returned\n    as well.\n\n    Args:\n        lat: Latitude of the central point in degrees.\n        lon: Longitude of the central point in degrees.\n        cube_width: The width of the cube in units of crs_final, used to define the\n            extent of the bounding box.\n        crs_final (pyproj.CRS or str, optional): The target CRS for the bounding box.\n            Defaults to \"utm\", which automatically determines the UTM zone based on the\n            latitude and longitude.\n\n    Returns:\n        A list of four integers representing the bounding box in the format\n            [west, south, east, north].\n        The final CRS used for the bounding box, returned as a `pyproj.CRS` object.\n    \"\"\"\n    if crs_final == \"utm\":\n        crs_final = get_utm_zone(lat, lon)\n    if isinstance(crs_final, str):\n        crs_final = pyproj.CRS.from_user_input(crs_final)\n\n    transformer = pyproj.Transformer.from_crs(CRS_WGS84, crs_final, always_xy=True)\n    x, y = transformer.transform(lon, lat)\n\n    half_size = cube_width / 2\n    bbox_final = [x - half_size, y - half_size, x + half_size, y + half_size]\n    if not crs_final.is_geographic:\n        bbox_final = [round(item) for item in bbox_final]\n    return bbox_final, crs_final\n</code></pre>"},{"location":"api/#xcube_multistore.utils.clean_dataset","title":"<code>xcube_multistore.utils.clean_dataset(ds)</code>","text":"<p>Cleans an xarray Dataset by removing boundary variables and normalizing the grid mapping.</p> <p>This function removes specific variables related to bounds (e.g., \"x_bnds\", \"y_bnds\", \"lat_bnds\", \"lon_bnds\", \"time_bnds\") and normalizes the grid mapping by adding a spatial reference coordinate called \"spatial_ref\" and assigning it to the relevant data variables.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input xarray dataset to be cleaned.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>A cleaned version of the dataset with boundary variables removed and grid</p> <code>Dataset</code> <p>mapping normalized.</p> Source code in <code>xcube_multistore/utils.py</code> <pre><code>def clean_dataset(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Cleans an xarray Dataset by removing boundary variables and normalizing the\n    grid mapping.\n\n    This function removes specific variables related to bounds (e.g., \"x_bnds\",\n    \"y_bnds\", \"lat_bnds\", \"lon_bnds\", \"time_bnds\") and normalizes the grid mapping\n    by adding a spatial reference coordinate called \"spatial_ref\" and assigning\n    it to the relevant data variables.\n\n    Args:\n        ds: The input xarray dataset to be cleaned.\n\n    Returns:\n        A cleaned version of the dataset with boundary variables removed and grid\n        mapping normalized.\n    \"\"\"\n    check_vars = [\"x_bnds\", \"y_bnds\", \"lat_bnds\", \"lon_bnds\", \"time_bnds\"]\n    sel_vars = []\n    for var in check_vars:\n        if var in ds:\n            sel_vars.append(var)\n    ds = ds.drop_vars(sel_vars)\n    ds = _normalize_grid_mapping(ds)\n    return ds\n</code></pre>"},{"location":"config/","title":"Configuration Guide","text":"<p>The Multi-Source Data Store is configured using a YAML file. This configuration  defines key elements such as datasets, data stores, grid mappings, and general  parameters, which are essential for setting up a workflow to generate data cubes.  This guide will walk you through the schema for each section of the configuration  file and show you how to create a valid configuration.</p> <p>To view the schema of the configuration, you can run the following code:</p> <pre><code>from xcube_multistore.multistore import MultiSourceDataStore\n\nMultiSourceDataStore.get_config_schema()\n</code></pre> <p>Additionally, you can explore example configurations in the  examples folder.</p>"},{"location":"config/#entire-configuration-schema","title":"Entire configuration schema","text":"<p>The entire configuration schema consists of the following sections: </p>"},{"location":"config/#properties","title":"Properties:","text":"<ul> <li>datasets (Required): list of dataset objects objects.</li> <li>preload_datasets (Optional): list of preload dataset objects.</li> <li>data_stores (Required): list of store objects.   A single data store with identifier <code>storage</code> is required for    writing the final data cubes.</li> <li>grid_mappings (Optional): list of grid mapping objects.</li> <li>general (Optional): following the general object mapping.</li> </ul>"},{"location":"config/#object-parameters","title":"Object parameters","text":"<p>An object in this configuration schema can refer to a dataset, data store, or grid  mapping. </p>"},{"location":"config/#dataset-object","title":"dataset object","text":"<p>Two different dataset types are available:</p>"},{"location":"config/#single-dataset-object","title":"single dataset object","text":"<p>This configures a dataset object from a single data source.  </p> <p>Properties: - identifier: This identifier defines the name of the final data cube. - store - grid_mapping - data_id - open_params - format_id - custom_processing</p>"},{"location":"config/#multi-dataset-object","title":"multi dataset object","text":"<p>This configures a datasets from multiple data sources. Each variable object is  opened individually, and once all datasets are loaded, they are merged using  <code>xarray.merge()</code>.</p> <p>Properties: - identifier: This identifier defines the name of the final data cube. - grid_mapping - format_id - xr_merge_params - variables: List of variable object objects.</p>"},{"location":"config/#variable-object","title":"variable object","text":"<p>This configures a variable object, which is stored as a data variable within a single  <code>xarray.Dataset</code> object.</p> <p>Properties: - identifier: This identifier defines the variable name within the <code>xarray.Dataset</code>. - store - data_id - open_params - custom_processing</p>"},{"location":"config/#store-object","title":"store object","text":"<p>This configures a store object representing one xcube data store instance.</p> <p>Properties: - identifier - store_id - store_params</p>"},{"location":"config/#grid-mapping-object","title":"grid mapping object","text":"<p>This configures a grid mapping object.</p> <p>Properties: - identifier - bbox - crs - spatial_res - tile_size - </p>"},{"location":"config/#preload-dataset-object","title":"preload dataset object","text":"<p>This object configures the preloading of datasets that require it. Note that some  datasets are not accessible instantly via lazy loading, due to storage in compressed  formats or due to lengthy queuing processes during data requests. These dataset can  be preloaded once, which are then available to the user throughout the lifespan of a project.</p> <p>Currently, the preload API is available for the following data stores: - clms - zenodo</p> <p>Properties: - store: The data store where the dataset is located. - data_ids: A list of data_id values for the datasets to preload. - preload_params: Parameters specific to the respective data store for preloading.</p>"},{"location":"config/#general-object","title":"general object","text":"<p>This object configures general parameters that control various aspects of the cube  generation process.</p> <p>Properties: - visualize - force_preload - dask_scheduler - gdal_http_params</p>"},{"location":"config/#single-field-parameters","title":"Single field parameters","text":""},{"location":"config/#identifier","title":"identifier","text":"<p>Identifier for any object like dataset, grid mapping, store.</p>"},{"location":"config/#store","title":"store","text":"<p>Store identifier used to open the dataset.</p>"},{"location":"config/#grid_mapping","title":"grid_mapping","text":"<p>Identifier that assigns a grid mapping to the final dataset for reprojection.</p>"},{"location":"config/#data_id","title":"data_id","text":"<p>Unique identifier for the dataset's data source within the assigned data store.</p>"},{"location":"config/#open_params","title":"open_params","text":"<p>Open data parameters related to the data store and data_id</p>"},{"location":"config/#xr_merge_params","title":"xr_merge_params","text":"<p>Parameters of <code>xarry.merge</code> needed if harmonization of multiple datasets into one datacube is required. For further information view the xarray.merge documentation. Defaults: <code>(compat=\"no_conflicts\", join=\"exact\", fill_value=\"&lt;NA&gt;\", combine_attrs=\"drop_conflicts\")</code>.</p>"},{"location":"config/#format_id","title":"format_id","text":"<p>Desired format of the saved datacube.  </p> <p>Default: <code>zarr</code> Allowed values: <code>netcdf</code>, <code>zarr</code></p>"},{"location":"config/#custom_processing","title":"custom_processing","text":"<p>This section enables users to define a Python function that is executed after opening  the data. It allows for custom dataset manipulation, making it particularly useful  for handling unstructured datasets. The function must accept an <code>xarray.Dataset</code> as  input and return a modified <code>xarray.Dataset</code> as output.</p> <p>Properties: - module_path: Path to the Python module relative to the config file. - function_name: Name of the function within the specified module. The function must   take an <code>xarray.Dataset</code> as input and return a transformed <code>xarray.Dataset</code>.  </p>"},{"location":"config/#store_id","title":"store_id","text":"<p>The store identifier is used within the xcube ecosystem to specify different data  stores. For more information about the available data stores, refer to the  xcube data store documentation, or check the README of each specific data store plugin on GitHub, where links are given  below.</p> <p>Allowed Values: - cds - clms - cmems - esa-cci, esa-cci-kc, esa-cci-zarr - file, https, memory, s3  - sentinelhub, sentinelhub-cdse - smos - stac, stac-cdse, stac-xcube - zenodo </p>"},{"location":"config/#store_params","title":"store_params","text":"<p>The store-specific parameters define the configuration for a particular data store.  These parameters can be retrieved using <code>get_data_store_params_schema(store_id)</code>.  For more details, refer to the xcube data store documentation,  or consult the README of each specific data store plugin on GitHub, which is linked in  the store_id section.</p>"},{"location":"config/#spatial_res","title":"spatial_res","text":"<p>Spatial Resolution of the final grid mapping.</p>"},{"location":"config/#crs","title":"crs","text":"<p>The Coordinate Reference System (CRS) defines the coordinate reference system  of the final grid mapping. Note that the CRS should be provided as a string,  e.g., \"EPSG:4326\".</p>"},{"location":"config/#bbox","title":"bbox","text":"<p>The bounding box of the final grid mapping, expressed in the units of the CRS, is  provided as [west, south, east, north].</p>"},{"location":"config/#tile_size","title":"tile_size","text":"<p>The spatial chunk size in the grid mapping. If a single integer is provided, a square  tile size is assumed. Otherwise, specify a list in the format [x, y] or [lon, lat].</p> <p>Default: [1024, 1024]</p>"},{"location":"config/#visualize","title":"visualize","text":"<p>Switch between visualization in table, if True, and logging, if False.  </p> <p>Default: <code>True</code></p>"},{"location":"config/#force_preload","title":"force_preload","text":"<p>If True, all data IDS given in section <code>preload_datasets</code> will be preloaded. If False,  only non-preloaded datasets will be preloaded.  </p> <p>Default: <code>True</code></p>"},{"location":"config/#dask_scheduler","title":"dask_scheduler","text":"<p>Scheduler mode put into <code>dask.config.set(scheduler=&lt;scheduler_mode&gt;)</code> </p> <p>Default: <code>threads</code> Allowed values: <code>threads</code>, <code>processes</code>, <code>single-threaded</code>, <code>sync</code>, <code>distributed</code></p>"},{"location":"config/#gdal_http_params","title":"gdal_http_params","text":"<p>GDAL http environment variables which are used when opening a tif file with  <code>rioxarray.open_rasterio</code>, which uses GDAL driver under the hood, from a remote source.  </p> <p>Properties: - gdal_http_max_retry: Maximal number of retries of an HTTP request in GDAL.    Default: <code>10</code>  - gdal_http_retry_delay :Delay in seconds between retries of an HTTP request in GDAL.    Default: <code>5</code> </p>"},{"location":"start/","title":"Getting Started","text":"<p>By installing the <code>xcube-multistore</code> package into an existing Python environment using</p> <pre><code>conda install -c conda-forge xcube-multistore\n</code></pre> <p>you are ready to go and use the Multi-Source Data Store as follows:</p> <pre><code>from xcube_multistore.multistore import MultiSourceDataStore\n\nmsds = MultiSourceDataStore(\"config.yml\")\n</code></pre>"}]}