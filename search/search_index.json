{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"xcube Multi-Source Data Store","text":"<p><code>xcube-multistore</code> is a Python package designed to create a Multi-Source Data Store  that enables the seamless integration of data from multiple sources into a unified  data model. This approach simplifies the data fusion process while ensuring  transparency and reproducibility through well-defined configurations.</p> <p>The package utilizes xcube\u2019s data access,  implemented via data store plugins, along with additional functionalities from  xcube, to manipulate and harmonize datasets according to user-defined specifications.</p> <p>The workflow includes the following steps:</p> <ol> <li>Data access through xcube data stores</li> <li>Data harmonization</li> <li>Data fusion (if required)</li> </ol> <p>This process results in either a single, unified data cube with all datasets aligned to a consistent grid or a catalog of separate datasets.</p>"},{"location":"#overview","title":"Overview","text":"<p>The Multi-Source Data Store is configured via a YAML file. You can find an example  configuration in examples/config.yml.</p> <p>For more detailed guidance on creating a configuration file, please refer to the  Configuration Guide.</p> <p>Once the configuration file is ready, the Multi-Source Data Store can be started  with a single line of code, as shown below:</p> <pre><code>from xcube_multistore.multistore import MultiSourceDataStore\n\nmsds = MultiSourceDataStore(\"config.yml\")\n</code></pre> <p>For further examples please view the examples folder.</p>"},{"location":"#features","title":"Features","text":"<p>IMPORTANT: The <code>xcube-multistore</code> package is currently in the early stages of development. The following features are available so far:</p> <ul> <li>subset of dataset (defined by grid mapping)</li> <li>resample and reproject dataset (defined by grid mapping)</li> <li>grid mapping may be defined by the user or by a dataset </li> <li>allow for time series at a single spatial point; interpolate the neighbouring points</li> <li>allow data fusion, where data variables in one <code>xr.Dataset</code> refers to different data sources</li> <li>support spatial cutout of an area around a defined spatial point.</li> <li>support preload API for xcube-clms and    xcube-zendoo</li> <li>allow to write to netcdf and zarr</li> </ul> <p>The following features will be implemented in the future:</p> <ul> <li>some auxiliary functionalities which shall help to setup a config yml file. </li> <li>interpolate along the time axis </li> </ul>"},{"location":"#license","title":"License","text":"<p>The package is open source and released under the  MIT license. </p>"},{"location":"about/","title":"About the <code>xcube-multistore</code> project","text":""},{"location":"about/#changelog","title":"Changelog","text":"<p>You can find the complete <code>xcube-multistore</code> changelog  here. </p>"},{"location":"about/#reporting","title":"Reporting","text":"<p>If you have suggestions, ideas, feature requests, or if you have identified a malfunction or error, then please  post an issue. </p>"},{"location":"about/#contributions","title":"Contributions","text":"<p>The <code>xcube-multistore</code> project welcomes contributions of any form as long as you respect our  code of conduct and follow our  contribution guide.</p> <p>If you'd like to submit code or documentation changes, we ask you to provide a  pull request (PR)  here.  For code and configuration changes, your PR must be linked to a  corresponding issue. </p>"},{"location":"about/#development","title":"Development","text":"<p>To install the <code>xcube-multistore</code> development environment into an existing Python  environment, do</p> <pre><code>pip install .[dev,doc]\n</code></pre> <p>or create a new environment using <code>conda</code> or <code>mamba</code></p> <pre><code>mamba env create \n</code></pre>"},{"location":"about/#testing-and-coverage","title":"Testing and Coverage","text":"<p><code>xcube-multistore</code> uses pytest for unit-level testing  and code coverage analysis.</p> <pre><code>pytest tests/ --cov=xarray_eopf --cov-report html\n</code></pre>"},{"location":"about/#code-style","title":"Code Style","text":"<p>The <code>xcube-multistore</code> source code is formatted and quality-controlled  using ruff:</p> <pre><code>ruff format\nruff check\n</code></pre>"},{"location":"about/#documentation","title":"Documentation","text":"<p>The <code>xcube-multistore</code> documentation is built using the  mkdocs tool.</p> <p>With repository root as current working directory:</p> <pre><code>pip install .[doc]\n\nmkdocs build\nmkdocs serve\nmkdocs gh-deploy\n</code></pre>"},{"location":"about/#license","title":"License","text":"<p><code>xcube-multistore</code> is open source made available under the terms and conditions of the  MIT license.</p>"},{"location":"api/","title":"Python API reference","text":""},{"location":"api/#xcube_multistoremultistore-module","title":"<code>xcube_multistore.multistore</code> module","text":"<p>The following described objects can be imported through the <code>xcube_multistore.multistore</code> module.</p>"},{"location":"api/#class-multisourcedatastore","title":"Class <code>MultiSourceDataStore</code>","text":""},{"location":"api/#xcube_multistore.multistore.MultiSourceDataStore","title":"<code>xcube_multistore.multistore.MultiSourceDataStore</code>","text":"<p>Manages access to multiple data sources and their configurations for generating data cubes.</p> <p>This class utilizes xcube data store plugins for data access, supports data harmonization, and enables visualization of data cube generation.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str | dict[str, Any]</code> <p>Configuration settings, provided as a dictionary or a string reference to a YAML configuration file.</p> required Notes <p>Detailed instructions on setting up the configuration can be found in the Configuration Guide.</p> Source code in <code>xcube_multistore/multistore.py</code> <pre><code>class MultiSourceDataStore:\n    \"\"\"Manages access to multiple data sources and their configurations for generating\n    data cubes.\n\n    This class utilizes xcube data store plugins for data access, supports data\n    harmonization, and enables visualization of data cube generation.\n\n    Args:\n        config: Configuration settings, provided as a dictionary or a string\n            reference to a YAML configuration file.\n\n    Notes:\n        Detailed instructions on setting up the configuration can be found in the\n        [Configuration Guide](https://xcube-dev.github.io/xcube-multistore/config/).\n    \"\"\"\n\n    def __init__(self, config: str | dict[str, Any]):\n        config = MultiSourceConfig(config)\n        self.config = config\n        self.stores = DataStores.setup_data_stores(config)\n        if config.grid_mappings:\n            self._grid_mappings = GridMappings.setup_grid_mappings(config)\n        else:\n            self._grid_mappings = None\n        self._states = {\n            identifier: GeneratorState(\n                identifier=identifier, status=GeneratorStatus.waiting\n            )\n            for identifier, config_ds in config.datasets.items()\n        }\n\n        # preload data, which is not preloaded as default\n        if config.preload_datasets is not None:\n            self._preload_datasets()\n\n        # generate data cubes\n        if self.config.general[\"visualize\"]:\n            self._display = GeneratorDisplay.create(list(self._states.values()))\n            self._display.display_title(\"Cube Generation\")\n            self._display.show()\n        self._generate_cubes()\n\n    @classmethod\n    def get_config_schema(cls) -&gt; JsonObjectSchema:\n        \"\"\"Retrieves the configuration schema for the multi-source data store.\n\n        Returns:\n            A schema object defining the expected structure of the configuration.\n        \"\"\"\n        return MultiSourceConfig.get_schema()\n\n    def _notify(self, event: GeneratorState):\n        state = self._states[event.identifier]\n        state.update(event)\n        if self.config.general[\"visualize\"]:\n            self._display.update()\n        else:\n            if event.status == GeneratorStatus.failed:\n                LOG.error(\"An error occurred: %s\", event.exception)\n            else:\n                LOG.info(event.message)\n\n    def _notify_error(self, identifier: str, exception: Any):\n        self._notify(\n            GeneratorState(\n                identifier,\n                status=GeneratorStatus.failed,\n                exception=exception,\n            )\n        )\n\n    def _preload_datasets(self):\n        for config_preload in self.config.preload_datasets:\n            store = getattr(self.stores, config_preload[\"store\"])\n\n            if self.config.general[\"force_preload\"]:\n                # preload all datasets again\n                data_ids_preloaded = []\n                data_ids = config_preload[\"data_ids\"]\n            else:\n                # filter preloaded data IDs\n                data_ids = []\n                data_ids_preloaded = []\n                for data_id_preload in config_preload[\"data_ids\"]:\n                    if all(\n                        store.cache_store.has_data(data_id)\n                        for data_id in self.config.preload_map[data_id_preload]\n                    ):\n                        data_ids_preloaded.append(data_id_preload)\n                    else:\n                        data_ids.append(data_id_preload)\n\n            # setup visualization\n            if self.config.general[\"visualize\"]:\n                display_preloaded = GeneratorDisplay.create(\n                    [\n                        GeneratorState(\n                            identifier=data_id,\n                            status=GeneratorStatus.stopped,\n                            message=\"Already preloaded.\",\n                        )\n                        for data_id in data_ids_preloaded\n                    ]\n                )\n                display_preloaded.display_title(\n                    f\"Preload Datasets from store {config_preload['store']!r}\"\n                )\n                if data_ids_preloaded:\n                    display_preloaded.show()\n            else:\n                LOG.info(f\"Preload Datasets from store {config_preload['store']!r}\")\n                for data_id in data_ids_preloaded:\n                    LOG.info(f\"Data ID {data_id!r} already preloaded.\")\n\n            if data_ids:\n                preload_params = config_preload.get(\"preload_params\", {})\n                if \"silent\" not in preload_params:\n                    preload_params[\"silent\"] = self.config.general[\"visualize\"]\n                _ = store.preload_data(*data_ids, **preload_params)\n\n    def _generate_cubes(self):\n        for identifier, config_ds in self.config.datasets.items():\n            data_id = _get_data_id(config_ds)\n            if getattr(self.stores, \"storage\").has_data(data_id):\n                self._notify(\n                    GeneratorState(\n                        identifier,\n                        status=GeneratorStatus.stopped,\n                        message=f\"Dataset {identifier!r} already generated.\",\n                    )\n                )\n                continue\n            self._notify(\n                GeneratorState(\n                    identifier,\n                    status=GeneratorStatus.started,\n                    message=f\"Open dataset {identifier!r}.\",\n                )\n            )\n            ds = self._open_dataset(config_ds)\n            if isinstance(ds, xr.Dataset):\n                self._notify(\n                    GeneratorState(\n                        identifier,\n                        message=f\"Processing dataset {identifier!r}.\",\n                    )\n                )\n            else:\n                self._notify_error(identifier, ds)\n                continue\n            ds = self._process_dataset(ds, config_ds)\n            if isinstance(ds, xr.Dataset):\n                self._notify(\n                    GeneratorState(\n                        identifier,\n                        message=f\"Write dataset {identifier!r}.\",\n                    )\n                )\n            else:\n                self._notify_error(identifier, ds)\n                continue\n            ds = self._write_dataset(ds, config_ds)\n            if isinstance(ds, xr.Dataset):\n                self._notify(\n                    GeneratorState(\n                        identifier,\n                        status=GeneratorStatus.stopped,\n                        message=f\"Dataset {identifier!r} finished.\",\n                    )\n                )\n            else:\n                store = getattr(self.stores, NAME_WRITE_STORE)\n                format_id = config_ds.get(\"format_id\", \"zarr\")\n                data_id = (\n                    f\"{config_ds['identifier']}.{MAP_FORMAT_ID_FILE_EXT[format_id]}\"\n                )\n                store.has_data(data_id) and store.delete_data(data_id)\n                self._notify_error(identifier, ds)\n\n    @_safe_execute()\n    def _open_dataset(self, config: dict) -&gt; xr.Dataset | Exception:\n        if \"data_id\" in config:\n            return self._open_single_dataset(config)\n        else:\n            dss = []\n            for config_var in config[\"variables\"]:\n                ds = self._open_single_dataset(config_var)\n                if len(ds.data_vars) &gt; 1:\n                    name_dict = {\n                        var: f\"{config_var[\"identifier\"]}_{var}\"\n                        for var in ds.data_vars.keys()\n                    }\n                else:\n                    name_dict = {\n                        var: f\"{config_var[\"identifier\"]}\"\n                        for var in ds.data_vars.keys()\n                    }\n                dss.append(ds.rename_vars(name_dict=name_dict))\n            merge_params = config.get(\"xr_merge_params\", {})\n            if \"join\" not in merge_params:\n                merge_params[\"join\"] = \"exact\"\n            if \"combine_attrs\" not in merge_params:\n                merge_params[\"combine_attrs\"] = \"drop_conflicts\"\n            ds = xr.merge(dss, **merge_params)\n        return clean_dataset(ds)\n\n    def _open_single_dataset(self, config: dict) -&gt; xr.Dataset | Exception:\n        store = getattr(self.stores, config[\"store\"])\n        open_params = copy.deepcopy(config.get(\"open_params\", {}))\n        lat, lon = open_params.pop(\"point\", [np.nan, np.nan])\n        schema = store.get_open_data_params_schema(data_id=config[\"data_id\"])\n        if (\n            ~np.isnan(lat)\n            and ~np.isnan(lon)\n            and \"bbox\" in schema.properties\n            and [\"spatial_res\"] in open_params\n            and \"spatial_res\" in schema.properties\n        ):\n            lat, lon = open_params.pop(\"point\")\n            open_params[\"bbox\"] = [\n                lon - 2 * open_params[\"spatial_res\"],\n                lat - 2 * open_params[\"spatial_res\"],\n                lon + 2 * open_params[\"spatial_res\"],\n                lat + 2 * open_params[\"spatial_res\"],\n            ]\n\n        if hasattr(store, \"cache_store\"):\n            try:\n                ds = store.cache_store.open_data(config[\"data_id\"], **open_params)\n            except Exception:\n                ds = store.open_data(config[\"data_id\"], **open_params)\n        else:\n            ds = store.open_data(config[\"data_id\"], **open_params)\n\n        # custom processing\n        if \"custom_processing\" in config:\n            module = importlib.import_module(config[\"custom_processing\"][\"module_path\"])\n            function = getattr(module, config[\"custom_processing\"][\"function_name\"])\n            ds = function(ds)\n\n        return clean_dataset(ds)\n\n    @_safe_execute()\n    def _process_dataset(self, ds: xr.Dataset, config: dict) -&gt; xr.Dataset | Exception:\n        # if grid mapping is given, resample the dataset\n        if \"grid_mapping\" in config:\n            if hasattr(self._grid_mappings, config[\"grid_mapping\"]):\n                target_gm = getattr(self._grid_mappings, config[\"grid_mapping\"])\n            else:\n                config_ref = self.config.datasets[config[\"grid_mapping\"]]\n                data_id = _get_data_id(config_ref)\n                ds_ref = getattr(self.stores, \"storage\").open_data(data_id)\n                target_gm = GridMapping.from_dataset(ds_ref)\n                for var_name, data_array in ds.items():\n                    if np.issubdtype(data_array.dtype, np.number):\n                        ds[var_name] = data_array.astype(target_gm.x_coords.dtype)\n            source_gm = GridMapping.from_dataset(ds)\n            transformer = pyproj.Transformer.from_crs(\n                target_gm.crs, source_gm.crs, always_xy=True\n            )\n            bbox = transformer.transform_bounds(*target_gm.xy_bbox, densify_pts=21)\n            bbox = [\n                bbox[0] - 2 * source_gm.x_res,\n                bbox[1] - 2 * source_gm.y_res,\n                bbox[2] + 2 * source_gm.x_res,\n                bbox[3] + 2 * source_gm.y_res,\n            ]\n\n            ds = clip_dataset_by_geometry(ds, geometry=bbox)\n            ds = resample_in_space(ds, target_gm=target_gm, encode_cf=True)\n            if \"time\" in ds.coords:\n                ds = chunk_dataset(\n                    ds, dict(time=1), format_name=config.get(\"format_id\", \"zarr\")\n                )\n\n        # if \"point\" in open_params, timeseries is requested\n        open_params = config.get(\"open_params\", {})\n        if \"point\" in open_params:\n            ds = ds.interp(\n                lat=open_params[\"point\"][0],\n                lon=open_params[\"point\"][1],\n                method=\"linear\",\n            )\n\n        return ds\n\n    @_safe_execute()\n    def _write_dataset(self, ds: xr.Dataset, config: dict) -&gt; xr.Dataset | Exception:\n        store = getattr(self.stores, NAME_WRITE_STORE)\n        format_id = config.get(\"format_id\", \"zarr\")\n        if format_id == \"netcdf\":\n            ds = prepare_dataset_for_netcdf(ds)\n        data_id = f\"{config['identifier']}.{MAP_FORMAT_ID_FILE_EXT[format_id]}\"\n        ds = clean_dataset(ds)\n        store.write_data(ds, data_id, replace=True)\n        return ds\n</code></pre>"},{"location":"api/#xcube_multistore.multistore.MultiSourceDataStore.get_config_schema","title":"<code>get_config_schema()</code>  <code>classmethod</code>","text":"<p>Retrieves the configuration schema for the multi-source data store.</p> <p>Returns:</p> Type Description <code>JsonObjectSchema</code> <p>A schema object defining the expected structure of the configuration.</p> Source code in <code>xcube_multistore/multistore.py</code> <pre><code>@classmethod\ndef get_config_schema(cls) -&gt; JsonObjectSchema:\n    \"\"\"Retrieves the configuration schema for the multi-source data store.\n\n    Returns:\n        A schema object defining the expected structure of the configuration.\n    \"\"\"\n    return MultiSourceConfig.get_schema()\n</code></pre>"},{"location":"api/#xcube_multistoreutils-module","title":"<code>xcube_multistore.utils</code> module","text":"<p>The following auxiliary functions can be imported through the <code>xcube_multistore.utils</code>  module.</p>"},{"location":"api/#function-prepare_dataset_for_netcdf","title":"Function <code>prepare_dataset_for_netcdf()</code>","text":""},{"location":"api/#xcube_multistore.utils.prepare_dataset_for_netcdf","title":"<code>xcube_multistore.utils.prepare_dataset_for_netcdf(ds)</code>","text":"<p>Prepares an xarray Dataset for NetCDF serialization.</p> <p>Converts non-serializable attributes (lists, tuples, and dictionaries) into strings to ensure compatibility with NetCDF format.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input xarray Dataset.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>A dataset with updated attributes, ensuring compatibility with NetCDF.</p> Source code in <code>xcube_multistore/utils.py</code> <pre><code>def prepare_dataset_for_netcdf(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Prepares an xarray Dataset for NetCDF serialization.\n\n    Converts non-serializable attributes (lists, tuples, and dictionaries) into strings\n    to ensure compatibility with NetCDF format.\n\n    Args:\n        ds: The input xarray Dataset.\n\n    Returns:\n        A dataset with updated attributes, ensuring compatibility with NetCDF.\n    \"\"\"\n    attrs = ds.attrs\n    for key in attrs:\n        if (\n            isinstance(attrs[key], list)\n            or isinstance(attrs[key], tuple)\n            or isinstance(attrs[key], dict)\n        ):\n            attrs[key] = str(attrs[key])\n    ds = ds.assign_attrs(attrs)\n    return ds\n</code></pre>"},{"location":"api/#function-get_utm_zone","title":"Function <code>get_utm_zone()</code>","text":""},{"location":"api/#xcube_multistore.utils.get_utm_zone","title":"<code>xcube_multistore.utils.get_utm_zone(lat, lon)</code>","text":"<p>Determines the UTM (Universal Transverse Mercator) zone for given coordinates.</p> <p>Computes the UTM zone based on longitude and returns the corresponding EPSG code. Northern hemisphere zones use EPSG codes in the 32600 range, while southern hemisphere zones use EPSG codes in the 32700 range.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude in degrees.</p> required <code>lon</code> <code>float</code> <p>Longitude in degrees.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The EPSG code for the corresponding UTM zone (e.g., \"epsg:32633\").</p> Source code in <code>xcube_multistore/utils.py</code> <pre><code>def get_utm_zone(lat: float, lon: float) -&gt; str:\n    \"\"\"Determines the UTM (Universal Transverse Mercator) zone for given coordinates.\n\n    Computes the UTM zone based on longitude and returns the corresponding EPSG code.\n    Northern hemisphere zones use EPSG codes in the 32600 range, while southern\n    hemisphere zones use EPSG codes in the 32700 range.\n\n    Args:\n        lat: Latitude in degrees.\n        lon: Longitude in degrees.\n\n    Returns:\n        The EPSG code for the corresponding UTM zone (e.g., \"epsg:32633\").\n    \"\"\"\n    zone_number = int((lon + 180) / 6) + 1\n    if lat &gt;= 0:\n        epsg_code = 32600 + zone_number\n    else:\n        epsg_code = 32700 + zone_number\n    return f\"epsg:{epsg_code}\"\n</code></pre>"},{"location":"api/#function-get_bbox","title":"Function <code>get_bbox()</code>","text":""},{"location":"api/#xcube_multistore.utils.get_bbox","title":"<code>xcube_multistore.utils.get_bbox(lat, lon, cube_width, crs_final='utm')</code>","text":"<p>Generates a bounding box around a specified latitude and longitude.</p> <p>Given a point (latitude, longitude) and the desired width of a cube, this function computes the bounding box in the specified coordinate reference system (CRS). The bounding box is returned as a list of coordinates, and the CRS is returned as well.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of the central point in degrees.</p> required <code>lon</code> <code>float</code> <p>Longitude of the central point in degrees.</p> required <code>cube_width</code> <code>float</code> <p>The width of the cube in units of crs_final, used to define the extent of the bounding box.</p> required <code>crs_final</code> <code>CRS or str</code> <p>The target CRS for the bounding box. Defaults to \"utm\", which automatically determines the UTM zone based on the latitude and longitude.</p> <code>'utm'</code> <p>Returns:</p> Type Description <code>(list[int], CRS)</code> <p>A list of four integers representing the bounding box in the format [west, south, east, north].</p> <code>(list[int], CRS)</code> <p>The final CRS used for the bounding box, returned as a <code>pyproj.CRS</code> object.</p> Source code in <code>xcube_multistore/utils.py</code> <pre><code>def get_bbox(\n    lat: float, lon: float, cube_width: float, crs_final: pyproj.CRS | str = \"utm\"\n) -&gt; (list[int], pyproj.CRS):\n    \"\"\"Generates a bounding box around a specified latitude and longitude.\n\n    Given a point (latitude, longitude) and the desired width of a cube, this function\n    computes the bounding box in the specified coordinate reference system (CRS).\n    The bounding box is returned as a list of coordinates, and the CRS is returned\n    as well.\n\n    Args:\n        lat: Latitude of the central point in degrees.\n        lon: Longitude of the central point in degrees.\n        cube_width: The width of the cube in units of crs_final, used to define the\n            extent of the bounding box.\n        crs_final (pyproj.CRS or str, optional): The target CRS for the bounding box.\n            Defaults to \"utm\", which automatically determines the UTM zone based on the\n            latitude and longitude.\n\n    Returns:\n        A list of four integers representing the bounding box in the format\n            [west, south, east, north].\n        The final CRS used for the bounding box, returned as a `pyproj.CRS` object.\n    \"\"\"\n    if crs_final == \"utm\":\n        crs_final = get_utm_zone(lat, lon)\n    if isinstance(crs_final, str):\n        crs_final = pyproj.CRS.from_user_input(crs_final)\n\n    transformer = pyproj.Transformer.from_crs(CRS_WGS84, crs_final, always_xy=True)\n    x, y = transformer.transform(lon, lat)\n\n    half_size = cube_width / 2\n    bbox_final = [x - half_size, y - half_size, x + half_size, y + half_size]\n    if not crs_final.is_geographic:\n        bbox_final = [round(item) for item in bbox_final]\n    return bbox_final, crs_final\n</code></pre>"},{"location":"api/#function-clean_dataset","title":"Function <code>clean_dataset()</code>","text":""},{"location":"api/#xcube_multistore.utils.clean_dataset","title":"<code>xcube_multistore.utils.clean_dataset(ds)</code>","text":"<p>Cleans an xarray Dataset by removing boundary variables and normalizing the grid mapping.</p> <p>This function removes specific variables related to bounds (e.g., \"x_bnds\", \"y_bnds\", \"lat_bnds\", \"lon_bnds\", \"time_bnds\") and normalizes the grid mapping by adding a spatial reference coordinate called \"spatial_ref\" and assigning it to the relevant data variables.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input xarray dataset to be cleaned.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>A cleaned version of the dataset with boundary variables removed and grid</p> <code>Dataset</code> <p>mapping normalized.</p> Source code in <code>xcube_multistore/utils.py</code> <pre><code>def clean_dataset(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Cleans an xarray Dataset by removing boundary variables and normalizing the\n    grid mapping.\n\n    This function removes specific variables related to bounds (e.g., \"x_bnds\",\n    \"y_bnds\", \"lat_bnds\", \"lon_bnds\", \"time_bnds\") and normalizes the grid mapping\n    by adding a spatial reference coordinate called \"spatial_ref\" and assigning\n    it to the relevant data variables.\n\n    Args:\n        ds: The input xarray dataset to be cleaned.\n\n    Returns:\n        A cleaned version of the dataset with boundary variables removed and grid\n        mapping normalized.\n    \"\"\"\n    check_vars = [\"x_bnds\", \"y_bnds\", \"lat_bnds\", \"lon_bnds\", \"time_bnds\"]\n    sel_vars = []\n    for var in check_vars:\n        if var in ds:\n            sel_vars.append(var)\n    ds = ds.drop_vars(sel_vars)\n    ds = _normalize_grid_mapping(ds)\n    return ds\n</code></pre>"},{"location":"config/","title":"Configuration Guide","text":"<p>The Multi-Source Data Store is configured using a YAML file. This configuration  defines key elements such as datasets, data stores, grid mappings, and general  parameters, which are essential for setting up a workflow to generate data cubes.  This guide will walk you through the schema for each section of the configuration  file and show you how to create a valid configuration.</p> <p>To view the schema of the configuration, you can run the following code:</p> <pre><code>from xcube_multistore.multistore import MultiSourceDataStore\n\nMultiSourceDataStore.get_config_schema()\n</code></pre> <p>Additionally, you can explore example configurations in the  examples folder.</p>"},{"location":"config/#configuration-schema","title":"Configuration Schema","text":"<p>Each item in the schema is represented by a user defined <code>identifier</code>, which is used  to identify the assigned object in the multi-source data store ecosystem. Furthermore,  the configuration file consists of the following sections: </p> <ul> <li><code>datasets</code> (Required): list of dataset objects.</li> <li><code>preload_dataset</code> (Optional): list of preload objects.</li> <li><code>data_store</code> (Required): list of data store objects.</li> <li><code>grid_mappings</code> (Optional): list of grid mapping objects.</li> <li><code>general</code> (Optional): mapping (dictionary) of general configuration parameters.</li> </ul>"},{"location":"config/#datasets-required","title":"<code>datasets</code> (Required)","text":"<p>The <code>datasets</code> section consists if a list of dataset configuration which defines the  source and the processing step to be applied to the dataset. Two different dataset  types are available:  1. Single Dataset Schema: represents a dataset from a single data source 2. Multi Dataset Schema: allows to merge multiple datasets from multiple  data sources into one dataset</p> <p>An example for the Single Dataset Schema is shown below: <pre><code>identifier: &lt;string&gt;\nstore: &lt;string&gt;\ndata_id: &lt;string&gt;\ngrid_mapping: &lt;string&gt;\nopen_params: &lt;object&gt;\nformat_id: &lt;string&gt;\ncustom_processing:\n  module_path: &lt;string&gt;\n  function_name: &lt;string&gt;\n</code></pre></p> <p>An example for the Multi Dataset Schema is shown below: <pre><code>identifier: &lt;string&gt;\ngrid_mapping: &lt;string&gt;\nformat_id: &lt;string&gt;\nxr_merge_params: &lt;dict&gt;\nvariables:\n    - identifier: &lt;string&gt;\n      store: &lt;string&gt;\n      data_id: &lt;string&gt;\n      open_params: &lt;dict&gt;\n      custom_processing:\n        module_path: &lt;string&gt;\n        function_name: &lt;string&gt;\n</code></pre></p>"},{"location":"config/#preload_dataset-optional","title":"<code>preload_dataset</code> (Optional)","text":""},{"location":"config/#data_store-required","title":"<code>data_store</code> (Required)","text":""},{"location":"config/#grid_mapping-optional","title":"<code>grid_mapping</code> (Optional)","text":""},{"location":"config/#general-optional","title":"<code>general</code> (Optional)","text":""},{"location":"start/","title":"Getting Started","text":"<p>By installing the <code>xcube-multistore</code> package into an existing Python environment using</p> <pre><code>pip install xcube-multistore\n</code></pre> <p>or</p> <pre><code>conda install -c conda-forge xcube-multistore\n</code></pre> <p>you are ready to go and use the Mutli-Source Data Store as follows:</p> <pre><code>from xcube_multistore import MultiSourceDataStore\n\nmsds = MultiSourceDataStore(\"config.yml\")\n</code></pre>"}]}